##
# RLlib configuration file for training a PPO agent in the pendulum environment.
# To start a training, type the following into the terminal:
#     >>> rllib train --config-file=/path/to/native-pendulum-ppo.yaml
# Depending on the software versions you have installed, you may need to edit
# l. 414 of ray/rllib/utils/pre_checks/env.py in your Python environment to
#     >>>     elif not isinstance(done, bool):
# so that RLlib doesn't crash.
#
# File:   native-pendulum-ppo.py
# Author: Kevin Badalian (badalian_k@mmp.rwth-aachen.de)
#         Teaching and Research Area Mechatronics in Mobile Propulsion (MMP)
#         RWTH Aachen University
# Date:   2023-10-18
#
#
# Copyright 2023 Teaching and Research Area Mechatronics in Mobile Propulsion,
#                RWTH Aachen University
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License. You may obtain a copy of
# the License at: http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations under
# the License.
#


native-pendulum-ppo:
    env: Pendulum-v1
    run: PPO
    stop:
        timesteps_total: 1000000
    config:
        # Config file parameters that are overwritten by the LExCI
        num_workers: 0
        rollout_fragment_length: 512
        framework: tf2
        # Config file parameters
        model:
            fcnet_hiddens: [64, 64]
            fcnet_activation: tanh
        train_batch_size: 512
        sgd_minibatch_size: 64
        num_sgd_iter: 6
        gamma: 0.95
        lambda: 0.1
        clip_param: 0.3
        vf_clip_param: 10000
        lr: 0.0003
        kl_target: 0.01
